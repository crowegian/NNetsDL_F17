{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: RNN application -- Tweet Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Plot configurations\n",
    "% matplotlib inline\n",
    "# Notebook auto reloads code. (Ref: http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython)\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of original train set: 60000\n",
      "size of original test set: 20000\n",
      "****************************************************************************************************\n",
      "size of train set: 60000, #positive: 30055, #negative: 29945\n",
      "size of test set: 1000, #positive: 510, #negative: 490\n",
      "['it', 'will', 'help', 'relieve', 'your', 'stress', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken', 'padtoken']\n",
      "sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "with open(\"./tweets_data/vocabulary.pkl\", \"rb\") as f:\n",
    "    vocabulary = pickle.load(f)\n",
    "\n",
    "# load our data and separate it into tweets and labels\n",
    "train_data = json.load(open('tweets_data/trainTweets_preprocessed.json', 'r'))\n",
    "train_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),train_data))\n",
    "train_tweets = np.array([t[0] for t in train_data])\n",
    "train_labels = np.array([int(t[1]) for t in train_data])\n",
    "\n",
    "test_data = json.load(open('tweets_data/testTweets_preprocessed.json', 'r'))\n",
    "test_data = list(map(lambda row:(np.array(row[0],dtype=np.int32),str(row[1])),test_data))\n",
    "test_tweets = np.array([t[0] for t in test_data])\n",
    "test_labels = np.array([int(t[1]) for t in test_data])\n",
    "\n",
    "print(\"size of original train set: {}\".format(len(train_tweets)))\n",
    "print(\"size of original test set: {}\".format(len(test_tweets)))\n",
    "\n",
    "# only select first 1000 test sample for test\n",
    "test_tweets = test_tweets[:1000]\n",
    "test_labels = test_labels[:1000]\n",
    "\n",
    "print(\"*\"*100)\n",
    "print(\"size of train set: {}, #positive: {}, #negative: {}\".format(len(train_tweets), np.sum(train_labels), len(train_tweets)-np.sum(train_labels)))\n",
    "print(\"size of test set: {}, #positive: {}, #negative: {}\".format(len(test_tweets), np.sum(test_labels), len(test_tweets)-np.sum(test_labels)))\n",
    "\n",
    "# show text of the idx-th train tweet\n",
    "# The 'padtoken' is used to ensure each tweet has the same length\n",
    "idx = 100\n",
    "train_text = [vocabulary[x] for x in train_tweets[idx]]\n",
    "print(train_text)\n",
    "sentiment_label = [\"negative\", \"positive\"]\n",
    "print(\"sentiment: {}\".format(sentiment_label[train_labels[idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LSTM Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a single-layer lstm network\n",
    "\n",
    "First of all, we'll build a single-layer LSTM network for the analysis. The network structure is the following:\n",
    "\n",
    "![](./img/singleLSTM.png)\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Below, we've defined the network for you. Please read the code carefully to make sure you understand it. Then, please write a training function to train this network. The settings are:\n",
    "\n",
    "1. Train the network for 1000 iterations. In each iteration, use batch_size samples to train the network.\n",
    "2. For every 50 iterations, apply the network on the test set, and print out the test accuracy and mean loss.\n",
    "\n",
    "With these settings, what accuracy could you get? You can try to change some stuff in the network to see if you could get a better accuracy (this is optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a linear layer, y = x*w + b\n",
    "def linear(input_, output_size, name, init_bias=0.0):\n",
    "    shape = input_.get_shape().as_list()\n",
    "    with tf.variable_scope(name):\n",
    "        init = tf.truncated_normal([shape[-1], output_size], mean=0.0, stddev=1.0 / shape[-1]**0.5)\n",
    "        W = tf.get_variable(\"weight\", initializer=init)\n",
    "    if init_bias is None:\n",
    "        return tf.matmul(input_, W)\n",
    "    with tf.variable_scope(name):\n",
    "        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n",
    "    return tf.matmul(input_, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set variables\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "batch_size = 64\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.int32, [None, tweet_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "tweets_onehot = tf.one_hot(tweets, depth=vocab_size, axis=-1)\n",
    "\n",
    "# define the lstm cell\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "# outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, initial_state=init_state, dtype=tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, dtype=tf.float32)\n",
    "\n",
    "# define that our final sentiment logit is a linear function of the final state of the LSTM\n",
    "sentiment = linear(final_state[-1], 1, name=\"output\")\n",
    "\n",
    "# define cross entropy/sigmoid loss function\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# compute accuracy\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "acc = tf.to_float(tf.equal(prediction, labels))\n",
    "acc = tf.reduce_mean(acc)\n",
    "\n",
    "# define optimizer\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_tweets = tf.convert_to_tensor(train_tweets)\n",
    "# train_labels = tf.convert_to_tensor(train_labels)\n",
    "# test_tweets = tf.convert_to_tensor(test_tweets)\n",
    "# test_labels = tf.convert_to_tensor(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_tweets = tf.convert_to_tensor(train_tweets)\n",
    "# train_labels = tf.convert_to_tensor(train_labels)\n",
    "# test_tweets = tf.convert_to_tensor(test_tweets)\n",
    "# test_labels = tf.convert_to_tensor(test_labels)\n",
    "def batchGenerator(x, y , batchSize, shuffle = False, seed = 23):\n",
    "    np.random.seed(seed)\n",
    "    nObs = x.shape[0]\n",
    "    totalBatches = nObs//batchSize\n",
    "#     print(totalBatches)\n",
    "#     1/0\n",
    "    #print(\"total batches {}\".format(totalBatches))\n",
    "    batchCount = 0\n",
    "    randIndices = np.random.randint(low = 0, high = nObs, size = nObs)\n",
    "    randIndices = np.random.permutation(nObs)\n",
    "    while True:\n",
    "#         print(batchCount)\n",
    "        if batchCount < totalBatches:\n",
    "            # batchCount += 1\n",
    "            myIndices = randIndices[batchSize*batchCount:(batchSize*(batchCount + 1))]\n",
    "            yield(x[myIndices,:], y[myIndices])\n",
    "            batchCount += 1\n",
    "        else:\n",
    "            if shuffle:\n",
    "#                 print('hey')\n",
    "#                 print(batchCount)\n",
    "                #print(\"Shuffling X\\nX shape before: {}\".format(self.x.shape))\n",
    "                myShuffle = np.random.permutation(nObs)\n",
    "                x = x[myShuffle, :]# shuffle x\n",
    "                y = y[myShuffle]\n",
    "            batchCount = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 0\n",
      "\tTest loss:0.6937750577926636\n",
      "\tTest Accurcy:0.492000013589859\n",
      "For Iteration 50\n",
      "\tTest loss:0.6652337908744812\n",
      "\tTest Accurcy:0.5999999642372131\n",
      "For Iteration 100\n",
      "\tTest loss:0.5711249113082886\n",
      "\tTest Accurcy:0.7199999690055847\n",
      "For Iteration 150\n",
      "\tTest loss:0.5664047002792358\n",
      "\tTest Accurcy:0.6950000524520874\n",
      "For Iteration 200\n",
      "\tTest loss:0.5575327277183533\n",
      "\tTest Accurcy:0.7239999771118164\n",
      "For Iteration 250\n",
      "\tTest loss:0.5398330092430115\n",
      "\tTest Accurcy:0.731999933719635\n",
      "For Iteration 300\n",
      "\tTest loss:0.5401647090911865\n",
      "\tTest Accurcy:0.7239999771118164\n",
      "For Iteration 350\n",
      "\tTest loss:0.5380051136016846\n",
      "\tTest Accurcy:0.7209999561309814\n",
      "For Iteration 400\n",
      "\tTest loss:0.5320348739624023\n",
      "\tTest Accurcy:0.7329999804496765\n",
      "For Iteration 450\n",
      "\tTest loss:0.523109495639801\n",
      "\tTest Accurcy:0.7430000305175781\n",
      "For Iteration 500\n",
      "\tTest loss:0.5253843665122986\n",
      "\tTest Accurcy:0.7410000562667847\n",
      "For Iteration 550\n",
      "\tTest loss:0.5377519726753235\n",
      "\tTest Accurcy:0.7310000061988831\n",
      "For Iteration 600\n",
      "\tTest loss:0.526928722858429\n",
      "\tTest Accurcy:0.746999979019165\n",
      "For Iteration 650\n",
      "\tTest loss:0.5189785957336426\n",
      "\tTest Accurcy:0.749000072479248\n",
      "For Iteration 700\n",
      "\tTest loss:0.5200147032737732\n",
      "\tTest Accurcy:0.7409999966621399\n",
      "For Iteration 750\n",
      "\tTest loss:0.5110792517662048\n",
      "\tTest Accurcy:0.7509999871253967\n",
      "For Iteration 800\n",
      "\tTest loss:0.5110059380531311\n",
      "\tTest Accurcy:0.7430000901222229\n",
      "For Iteration 850\n",
      "\tTest loss:0.5268848538398743\n",
      "\tTest Accurcy:0.7550000548362732\n",
      "For Iteration 900\n",
      "\tTest loss:0.5076776146888733\n",
      "\tTest Accurcy:0.752000093460083\n",
      "For Iteration 950\n",
      "\tTest loss:0.5219939351081848\n",
      "\tTest Accurcy:0.7500000596046448\n",
      "For Iteration 999\n",
      "\tTest loss:0.5097923874855042\n",
      "\tTest Accurcy:0.7500000596046448\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "num_steps = 1000\n",
    "myBatchGen = batchGenerator(x = train_tweets, y = train_labels, batchSize = batch_size, shuffle = False, seed = 23)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print('1')\n",
    "    for step in range(num_steps):\n",
    "        # TODO: get data for a batch\n",
    "        training_batch_x, training_batch_y = next(myBatchGen)\n",
    "\n",
    "        # TODO: run the 'optimizer', 'loss', and 'acc' operations in the graph using the batch data\n",
    "        _, train_loss, train_acc = sess.run([optimizer, loss, acc], feed_dict={tweets: training_batch_x,\n",
    "                                                                               labels: training_batch_y})\n",
    "#         _, train_loss, train_acc = pass\n",
    "#         if step > 230:\n",
    "#             print(step)\n",
    "        if (step % 50 == 0 or step == 999):\n",
    "            trainLoss, testAccuracy = sess.run([loss, acc], feed_dict={tweets: test_tweets,\n",
    "                                                                           labels: test_labels})\n",
    "            print(\"For Iteration {}\\n\\tTest loss:{}\\n\\tTest Accurcy:{}\".format(step, trainLoss, testAccuracy))\n",
    "#             # TODO: get test accuracy and loss, and print them out.\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train a two-layer lstm network\n",
    "\n",
    "Next, we look at a slightly more difficult network structure: a double-layer LSTM. The output of the first LSTM cell is propagated to the second LSTM cell. We only need to make small modifications to the previous network to construct this one.\n",
    "\n",
    "![](./img/doubleLSTM.png)\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Define this double-layer LSTM in the below notebook cell. You can copy the single-layer network code and make changes to it. After that, train the network using the function you just wrote.\n",
    "\n",
    "Hint: Use **tf.contrib.rnn.MultiRNNCell**. You'll find there are only 2-3 lines of code that need to be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR DOUBLE-LAYER LSTM Here\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "batch_size = 64\n",
    "nLayers = 2\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.int32, [None, tweet_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "tweets_onehot = tf.one_hot(tweets, depth=vocab_size, axis=-1)\n",
    "\n",
    "# define the lstm cell\n",
    "# lstm_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "lstm_cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(hidden_size) for _ in range(nLayers)])\n",
    "# print(lstm_cell)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "# outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, initial_state=init_state, dtype=tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, dtype=tf.float32)\n",
    "\n",
    "# define that our final sentiment logit is a linear function of the final state of the LSTM\n",
    "# print(final_state[-1])\n",
    "# print(final_state[-1][-1])\n",
    "sentiment = linear(final_state[-1][-1], 1, name=\"output\")\n",
    "\n",
    "# define cross entropy/sigmoid loss function\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# compute accuracy\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "acc = tf.to_float(tf.equal(prediction, labels))\n",
    "acc = tf.reduce_mean(acc)\n",
    "\n",
    "# define optimizer\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 0\n",
      "\tTest loss:0.694391667842865\n",
      "\tTest Accurcy:0.49000000953674316\n",
      "For Iteration 50\n",
      "\tTest loss:0.672762393951416\n",
      "\tTest Accurcy:0.5769999623298645\n",
      "For Iteration 100\n",
      "\tTest loss:0.5861077904701233\n",
      "\tTest Accurcy:0.6920000314712524\n",
      "For Iteration 150\n",
      "\tTest loss:0.5694620013237\n",
      "\tTest Accurcy:0.7109999656677246\n",
      "For Iteration 200\n",
      "\tTest loss:0.543725848197937\n",
      "\tTest Accurcy:0.7269999980926514\n",
      "For Iteration 250\n",
      "\tTest loss:0.5420609712600708\n",
      "\tTest Accurcy:0.7279999256134033\n",
      "For Iteration 300\n",
      "\tTest loss:0.541493833065033\n",
      "\tTest Accurcy:0.7360000610351562\n",
      "For Iteration 350\n",
      "\tTest loss:0.5385265946388245\n",
      "\tTest Accurcy:0.7359999418258667\n",
      "For Iteration 400\n",
      "\tTest loss:0.5183773040771484\n",
      "\tTest Accurcy:0.7380000352859497\n",
      "For Iteration 450\n",
      "\tTest loss:0.5257933139801025\n",
      "\tTest Accurcy:0.7490000128746033\n",
      "For Iteration 500\n",
      "\tTest loss:0.518266499042511\n",
      "\tTest Accurcy:0.7470000386238098\n",
      "For Iteration 550\n",
      "\tTest loss:0.5462899804115295\n",
      "\tTest Accurcy:0.718999981880188\n",
      "For Iteration 600\n",
      "\tTest loss:0.5126178860664368\n",
      "\tTest Accurcy:0.7430000305175781\n",
      "For Iteration 650\n",
      "\tTest loss:0.5112797617912292\n",
      "\tTest Accurcy:0.7509999871253967\n",
      "For Iteration 700\n",
      "\tTest loss:0.5120583176612854\n",
      "\tTest Accurcy:0.7519999742507935\n",
      "For Iteration 750\n",
      "\tTest loss:0.5107935667037964\n",
      "\tTest Accurcy:0.7549999952316284\n",
      "For Iteration 800\n",
      "\tTest loss:0.5062886476516724\n",
      "\tTest Accurcy:0.7529999613761902\n",
      "For Iteration 850\n",
      "\tTest loss:0.5257524251937866\n",
      "\tTest Accurcy:0.7579999566078186\n",
      "For Iteration 900\n",
      "\tTest loss:0.5027849078178406\n",
      "\tTest Accurcy:0.753000020980835\n",
      "For Iteration 950\n",
      "\tTest loss:0.5230821967124939\n",
      "\tTest Accurcy:0.7610000371932983\n",
      "For Iteration 999\n",
      "\tTest loss:0.5133156776428223\n",
      "\tTest Accurcy:0.7500000596046448\n"
     ]
    }
   ],
   "source": [
    "# YOUR TRAINING HERE\n",
    "num_steps = 1000\n",
    "myBatchGen = batchGenerator(x = train_tweets, y = train_labels, batchSize = batch_size, shuffle = False, seed = 23)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "#     print('1')\n",
    "    for step in range(num_steps):\n",
    "        # TODO: get data for a batch\n",
    "        training_batch_x, training_batch_y = next(myBatchGen)\n",
    "\n",
    "        # TODO: run the 'optimizer', 'loss', and 'acc' operations in the graph using the batch data\n",
    "        _, train_loss, train_acc = sess.run([optimizer, loss, acc], feed_dict={tweets: training_batch_x,\n",
    "                                                                               labels: training_batch_y})\n",
    "#         _, train_loss, train_acc = pass\n",
    "#         if step > 230:\n",
    "#             print(step)\n",
    "        if (step % 50 == 0 or step == 999):\n",
    "            trainLoss, testAccuracy = sess.run([loss, acc], feed_dict={tweets: test_tweets,\n",
    "                                                                           labels: test_labels})\n",
    "            print(\"For Iteration {}\\n\\tTest loss:{}\\n\\tTest Accurcy:{}\".format(step, trainLoss, testAccuracy))\n",
    "#             # TODO: get test accuracy and loss, and print them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Embedding Lookup layer\n",
    "\n",
    "![](./img/embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Define an embedding layer\n",
    "\n",
    "It's not hard to imagine in the previous practices, the input we fed in are very sparse because each word was represented as a one-hot vector. This makes it difficult for the network to understand what story the input data is telling. \n",
    "\n",
    "Word embedding: instead of using a one-hot vector to represent each word, we can add an word embedding matrix in which each word is represented as a low-dimensional vector. Note that this representation is not sparse any more, because we're working in a continuous vector space now. Words that share similar/related semantic meaning should be 'close to each other' in this vector space (we could define a distance measure to estimate the closeness). \n",
    "\n",
    "[https://www.tensorflow.org/tutorials/word2vec](https://www.tensorflow.org/tutorials/word2vec)\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Take a look at the website above, and write a function to do the embedding. The website itself is a very good tutorial.\n",
    "\n",
    "Hint: Use **tf.nn.embedding_lookup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding(input_, vocab_size, output_size, name):\n",
    "    \"\"\"\n",
    "    1. Define an embedding matrix\n",
    "    2. return both the lookup results and the embedding matrix.\n",
    "    \"\"\"\n",
    "#     embeddings = np.array(np.random.randn(len(helper.tok2id) + 1, embeddingSize), dtype=np.float32)\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocab_size, output_size], -1.0, 1.0))# embeddings\n",
    "    # are random uniform between -1 and 1\n",
    "    embeddingLookup = tf.nn.embedding_lookup(params = embeddings, ids = input_,\n",
    "                                            name = name)\n",
    "    return(embeddings, embeddingLookup)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a single lstm network with embedding layer\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span> Build a single-layer LSTM network according to the network structure. Then, train the network with the training function you wrote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 20, 7597)\n",
      "(?, 20, 50)\n"
     ]
    }
   ],
   "source": [
    "print(tweets_onehot.get_shape())\n",
    "print(tweetsLookUps.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR EMBEDDING SINGLE-LAYER LSTM HERE\n",
    "tweet_size = 20\n",
    "hidden_size = 100\n",
    "vocab_size = 7597\n",
    "batch_size = 64\n",
    "embeddingSize = 50\n",
    "\n",
    "# this just makes sure that all our following operations will be placed in the right graph.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "# make placeholders for data we'll feed in\n",
    "tweets = tf.placeholder(tf.int32, [None, tweet_size])\n",
    "labels = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "tweets_onehot = tf.one_hot(tweets, depth=vocab_size, axis=-1)\n",
    "tweetsWordEmbeddings, tweetsLookUps = embedding(input_ = tweets, vocab_size = vocab_size,\n",
    "                                                output_size = embeddingSize, name = \"myEmbeddings\")\n",
    "\n",
    "# define the lstm cell\n",
    "lstm_cell = tf.contrib.rnn.LSTMCell(hidden_size)\n",
    "\n",
    "# define the op that runs the LSTM, across time, on the data\n",
    "init_state = lstm_cell.zero_state(batch_size, tf.float32)\n",
    "# outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweets_onehot, initial_state=init_state, dtype=tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, tweetsLookUps, dtype=tf.float32)\n",
    "\n",
    "# define that our final sentiment logit is a linear function of the final state of the LSTM\n",
    "sentiment = linear(final_state[-1], 1, name=\"output\")\n",
    "\n",
    "# define cross entropy/sigmoid loss function\n",
    "sentiment = tf.squeeze(sentiment, [1])\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=sentiment, labels=labels)\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# compute accuracy\n",
    "prob = tf.nn.sigmoid(sentiment)\n",
    "prediction = tf.to_float(tf.greater_equal(prob, 0.5))\n",
    "acc = tf.to_float(tf.equal(prediction, labels))\n",
    "acc = tf.reduce_mean(acc)\n",
    "\n",
    "# define optimizer\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "gradients = trainer.compute_gradients(loss)\n",
    "gradients_clipped = [(tf.clip_by_value(t[0],-1,1),t[1]) for t in gradients]\n",
    "optimizer = trainer.apply_gradients(gradients_clipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Iteration 0\n",
      "\tTest loss:0.7091147899627686\n",
      "\tTest Accurcy:0.46699997782707214\n",
      "For Iteration 50\n",
      "\tTest loss:0.6875331997871399\n",
      "\tTest Accurcy:0.5519999861717224\n",
      "For Iteration 100\n",
      "\tTest loss:0.6766270399093628\n",
      "\tTest Accurcy:0.5870000123977661\n",
      "For Iteration 150\n",
      "\tTest loss:0.6714581847190857\n",
      "\tTest Accurcy:0.5919999480247498\n",
      "For Iteration 200\n",
      "\tTest loss:0.6522839665412903\n",
      "\tTest Accurcy:0.6279999613761902\n",
      "For Iteration 250\n",
      "\tTest loss:0.6509639620780945\n",
      "\tTest Accurcy:0.6280000805854797\n",
      "For Iteration 300\n",
      "\tTest loss:0.6340239644050598\n",
      "\tTest Accurcy:0.6559999585151672\n",
      "For Iteration 350\n",
      "\tTest loss:0.6047096252441406\n",
      "\tTest Accurcy:0.6779999732971191\n",
      "For Iteration 400\n",
      "\tTest loss:0.6077768802642822\n",
      "\tTest Accurcy:0.6819999814033508\n",
      "For Iteration 450\n",
      "\tTest loss:0.589460015296936\n",
      "\tTest Accurcy:0.6840000152587891\n",
      "For Iteration 500\n",
      "\tTest loss:0.577507734298706\n",
      "\tTest Accurcy:0.6990000009536743\n",
      "For Iteration 550\n",
      "\tTest loss:0.5932913422584534\n",
      "\tTest Accurcy:0.6970000267028809\n",
      "For Iteration 600\n",
      "\tTest loss:0.5634082555770874\n",
      "\tTest Accurcy:0.7120000123977661\n",
      "For Iteration 650\n",
      "\tTest loss:0.5573297142982483\n",
      "\tTest Accurcy:0.7230000495910645\n",
      "For Iteration 700\n",
      "\tTest loss:0.5718936324119568\n",
      "\tTest Accurcy:0.7079999446868896\n",
      "For Iteration 750\n",
      "\tTest loss:0.5504059791564941\n",
      "\tTest Accurcy:0.7330000400543213\n",
      "For Iteration 800\n",
      "\tTest loss:0.5396782159805298\n",
      "\tTest Accurcy:0.7350000143051147\n",
      "For Iteration 850\n",
      "\tTest loss:0.5380865931510925\n",
      "\tTest Accurcy:0.7379999756813049\n",
      "For Iteration 900\n",
      "\tTest loss:0.5419049859046936\n",
      "\tTest Accurcy:0.7440000176429749\n",
      "For Iteration 950\n",
      "\tTest loss:0.5347182750701904\n",
      "\tTest Accurcy:0.7429999709129333\n",
      "For Iteration 999\n",
      "\tTest loss:0.5378749966621399\n",
      "\tTest Accurcy:0.7409999966621399\n",
      "For Iteration 1000\n",
      "\tTest loss:0.5361065864562988\n",
      "\tTest Accurcy:0.7419999837875366\n",
      "For Iteration 1050\n",
      "\tTest loss:0.528207540512085\n",
      "\tTest Accurcy:0.7339999675750732\n",
      "For Iteration 1100\n",
      "\tTest loss:0.5321202874183655\n",
      "\tTest Accurcy:0.7420000433921814\n",
      "For Iteration 1150\n",
      "\tTest loss:0.5250081419944763\n",
      "\tTest Accurcy:0.7549999356269836\n",
      "For Iteration 1200\n",
      "\tTest loss:0.5287905931472778\n",
      "\tTest Accurcy:0.7310000658035278\n",
      "For Iteration 1250\n",
      "\tTest loss:0.5314377546310425\n",
      "\tTest Accurcy:0.7459999918937683\n",
      "For Iteration 1300\n",
      "\tTest loss:0.5206174254417419\n",
      "\tTest Accurcy:0.7540000081062317\n",
      "For Iteration 1350\n",
      "\tTest loss:0.5179265737533569\n",
      "\tTest Accurcy:0.7489998936653137\n",
      "For Iteration 1400\n",
      "\tTest loss:0.5184128880500793\n",
      "\tTest Accurcy:0.7559999823570251\n",
      "For Iteration 1450\n",
      "\tTest loss:0.5160544514656067\n",
      "\tTest Accurcy:0.7540000081062317\n"
     ]
    }
   ],
   "source": [
    "# YOUR TRAINING HERE\n",
    "num_steps = 1500\n",
    "myBatchGen = batchGenerator(x = train_tweets, y = train_labels, batchSize = batch_size, shuffle = False, seed = 23)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    trainedEmbeddingsInit = tweetsWordEmbeddings.eval()\n",
    "#     print('1')\n",
    "    for step in range(num_steps):\n",
    "        # TODO: get data for a batch\n",
    "        training_batch_x, training_batch_y = next(myBatchGen)\n",
    "\n",
    "        # TODO: run the 'optimizer', 'loss', and 'acc' operations in the graph using the batch data\n",
    "        _, train_loss, train_acc = sess.run([optimizer, loss, acc], feed_dict={tweets: training_batch_x,\n",
    "                                                                               labels: training_batch_y})\n",
    "#         _, train_loss, train_acc = pass\n",
    "#         if step > 230:\n",
    "#             print(step)\n",
    "        if (step % 50 == 0 or step == 999):\n",
    "            trainLoss, testAccuracy = sess.run([loss, acc], feed_dict={tweets: test_tweets,\n",
    "                                                                           labels: test_labels})\n",
    "            print(\"For Iteration {}\\n\\tTest loss:{}\\n\\tTest Accurcy:{}\".format(step, trainLoss, testAccuracy))\n",
    "#             # TODO: get test accuracy and loss, and print them out.\n",
    "    trainedEmbeddingsFinal = tweetsWordEmbeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00091599597"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainedEmbeddingsInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0011279726"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(trainedEmbeddingsFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize word vectors via tSNE\n",
    "\n",
    "<span style=\"color:red\">__TODO:__</span>\n",
    "\n",
    "First, you need to retrieve **embedding matrix** from the network. Then use tSNE to reduce each low-dimensional word vector into a 2D vector. \n",
    "\n",
    "And then, you should visualize some interesting word pairs in 2D panel. You may find **scatter** function in **matplotlib.pyplot** useful.\n",
    "\n",
    "\n",
    "Hint: You can use **TSNE** tool provided in **scikit-learn**. And if you encounter dead kernel problem caused by \"Intel MKL FATAL ERROR: Cannot load libmkl_avx.so or libmkl_def.so\", please reinstall scikit-learn without MKL, ie., **conda install nomkl numpy scipy scikit-learn numexpr**. \n",
    "\n",
    "Here we provide some word pairs for you, like female-male or country-capital. And you can observe that these word-pair will look parallel with each other in a 2D tSNE panel. And you can find some other words and explore their relationship.\n",
    "\n",
    "The result for female-male pairs should look like, and you will observe that king-men and queen-women are parallel to each other in a 2D panel.\n",
    "\n",
    "\n",
    "![](./img/tsne_female_male.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabDict = {}\n",
    "for idx, word in enumerate(vocabulary):\n",
    "    vocabDict[word] = idx\n",
    "def lookupWordVec(word, vocab = vocabDict, embeddingsMat = trainedEmbeddingsFinal):\n",
    "    return(embeddingsMat[vocab[word],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "female_male = [\"men\", \"women\", \"king\", \"queen\"]\n",
    "country_capital = [\"spain\", \"madrid\", \"italy\", \"rome\", \"japan\", \"tokyo\"]\n",
    "# you can try some other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = np.array([lookupWordVec(word = \"man\"), lookupWordVec(word = \"women\"),\n",
    "              lookupWordVec(word = \"king\"), lookupWordVec(word = \"queen\")])\n",
    "# X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "dimReducVecs_gender = TSNE(n_components=2).fit_transform(X)\n",
    "# dimReducVecs_gender = np.vstack([dimReducVecs, dimReducVecs[2,:] - dimReducVecs[0,:] + dimReducVecs[1,:]])\n",
    "# dimReducVecs = np.transpose(dimReducVecs)\n",
    "### your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEMCAYAAADeYiHoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXJ8kQEkCCBAyJYBAhiOyJgFIRq17wumGk\nYu39CWqvdddfr9S11qVabwWvrQuKrVuLC/5kEVERRLiKWgjIKmE1BMK+hIAJITP5/v7IRAMOazJz\nZpL38/HIg5nvOWe+nxwO8+ac75nvmHMOCc3MmgHNgXivazlIANjjnNvtdSEiUn+ZAuKnfD47o2NH\nLmndmqyMDCp9Pq8rOlBFBWzaRNzWraxZs4aPysrcAq9rEpH6RwFxkKQk633eedwxciQl/fuzq1Ej\nonIH+f3Yv/5F86ef5sRPPuHlPXvc517XJCL1iwKiBjNrlJPDX996i92nnUaZ1/UcjaIiEocNo9Wc\nOdzlnNvrdT0iUn/EeV1AlOnUrx+NYiUcADIyKB84kLj4eE73uhYRqV8UEDU0bcrJvXt7XcWx69GD\nirQ0Onhdh4jULwqIGpo0IblxYwJe13GskpMJJCXRxOs6RKR+UUAcxCw6B6UPJxZrFpHop4AQEZGQ\nFBBHYcQIzr7xRvoCXHQRgzp1YjjAmDG079eP3AcfpGubNtyclsYtl13GBdXbJSZy/yWXcGFaGrd0\n7sy1b71FRseOjGjZkjuffJIsgP37sUsu4cJ27fjP9HRuvv12sgHGjiWzY0dGZGdzVevW3NavH7mV\nlV789iLSUCkgjsIFF1A4fz7tAFauJL28nEalpcR99hnt2rVjx3PPceGsWbxeUMCL+flkPPEEnQH2\n78f385/z3ebNvNC4MeWPPsrPFy3ijb//nbeffZbzAO6+m97NmlFeWMjL+fmMnTSJ7DlzSAFYv560\nsWP5eONGnt+8mRavv15Vg4hIJCggjkJuLhsLCkgvKiLR5yPQuTPr33uP9MWLOaV5c/Z16UJBVhal\njRtTefHFLP7iC04BiI8ncNddrAY47TS29upFQXIylZdcwtadO6tCYM4cOsyYQY+MDG46/XT+8/vv\nSZo/n5YAmZkUZWdTkpCAO/VUNq9cWbWNiEgkJHhdQCxITqYyNZVdjzxCz65dWd+9O1s+/pj2W7dy\n4qmnUrxsGemhtouPpzIuGMFxcbhGjarukEpIwFVWVoWzc/Dgg3x4552sqbnt2LFk+nw/3lEVH4+r\nqFCgi0jk6A3nKPXoQeH48Zw9cCDrrrySdVOnktOuHZsuvpii5cs5ZdUqkvfvxz76iG7nnEPB0b7u\nz37Gmldf5czS0qq/i5kzabl1K1E2+5OINEQ6gzhKAwey7r33OOeqq1jfujUVPh/+3r0p7N6dvbfe\nyowBAxjuHNanDyvvu48VR/u6Tz/NgqFDSenQgd84h51wAt9/+ilvh/N3ERE5GpqLqYa0NMt95hkG\nX301G72u5Vh8+CGt7ryTBatWub97XYuI1B+6xFRDIEDA74+9feL3E1dZid/rOkSkfom5N8Nw2rWL\nnVu3Rt2XAx3Rli002rmTHV7XISL1iwKihkCAVTNnQqx9IO2zz7DiYpZ5XYeI1C8KiANtyc/n2+ef\np22shMTrr5M+fz6FwDqvaxGR+kWD1Acxsyanncad55zDaeefj79tW8oaNSKq4sLvJ27DBpJmzSJh\n9mw25OczWt9PLSJ1TQERgpklAKemp9MrJYU2cXE08rqmmiorqSgpYeuGDSwAVjvnKryuSUTqHwVE\njDGzXwMrnXP/63UtIlK/aQwi9hQCb5nZc2bW1OtiRKT+UkDEGOfcJ0BXoAmwxMwu9LgkEamndIkp\nhpnZIGAsMB242zlX7HFJIlKPxFRApKamuszMTK/LiCqBQICioiKKi4tp164dKSmaEVxEDjR//vzt\nzrlWx7pdTE3Wl5mZSV5entdlRKVZs2bx61//ms6dO/PMM8+QmprqdUkiEiXM7Lg+J6UxiHpi4MCB\nLFq0iNatW9OtWzfeffddr0sSkRingKhHmjRpwtNPP82ECRN46KGHGDp0KJs3b/a6LBGJUQqIeuis\ns87im2++ISsrix49evCPf/wD5xzbt2/nhhtuIJbGnUTEOwqIeqpx48Y8/vjjfPTRR4waNYpLLrmE\n0tJSFi1axLhx47wuT0RiQEQCwswKzGyJmS00s7xg24lmNt3MVgX/bBGJWhqa3r17M2/ePPr27Ut2\ndjYXXHAB9957L3v37vW6NBGJcpE8gzjPOdfTOZcTfH4v8KlzriPwafC5hMH27dtp0aIFDz74IFOm\nTGHfvn3cc889TPqmiP5PzqT9vVPp/+RMJn1T5HWpIhJFvLzN9XJgYPDx68As4B6viqnPKioqWLly\nJYWFhfh8PkpLSxnz4ot82uJi9vmrxiOKisu4b8ISAIb0yvCyXBGJEhH5oJyZfQfsBgLAS865sWZW\n7JxLCS43YFf184O2vRG4EaBdu3bZ69bpaw/qwtlPTGdjyf6ftGekJDHn3p97UJGIhIuZza9x9eao\nReoM4mfOuSIzaw1MN7P8mgudc87MQiaVc24sVdNJkJOTo9tv6simEOEAsLG4LMKViEi0isgYhHOu\nKPjnVmAi0AfYYmZtAIJ/bo1ELVIlPSXpmNpFpOEJe0CYWRMza1b9GPg3YCnwPjA8uNpwYHK4a5Ef\njRyURZIv/oC2JF88IwdleVSRiESbSFxiOgmYWDXMQALwpnPuYzObB4w3sxuo+j7lqyJQiwRVD0Q/\nNW0FG4vLSE9JYuSgLA1Qi8gPYmo215ycHKfJ+kREjs3xDlLrk9QiIhKSAkJEREJSQIiISEgKCBER\nCUkBISIiIcXUV46KiNQFv99PQUEBe/bsobKyMix9+Hw+UlNTadOmDcHb/GOOAkJEGozKykqmTZvM\nokWfcNJJ5aSkQHz8kbc7HhUVMHu2IxBozQUX/Addu3YLT0dhpIAQkQbBOceECePYt286t912Mk2b\nNopIn0VFe3jnndE491u6dese9j7rksYgRKRB2Lx5Mxs2fMrVV2dGJBwAzIyTTz6BYcNO5NNP/xlz\nX/ergBCRBuHbbxfTtauRkBD5t72MjGbEx29j48aNEe+7NhQQItIgbNtWQHp6sid9mxkZGcb27ds9\n6f94KSBEpEHw+8vx+bx7y/P5qr7dMZYoIESkwfDydtNYvNNVASEiIiHpNlcRafAKCooZPPif9Ot3\nMl9+uZ4zz8zguut68oc/zGLr1u8ZNy4XgDvv/Jh9+/wkJSXw6quXk5WVymuvLeT991dQWlrBmjW7\nuOKKzvz5zxd6/BvVDZ1BiIgAq1fv5L/+6yzy828jP387b765hC++uI5Roy7kiSc+p3PnVD7//Dq+\n+eY3PProedx//8wftl24cDPvvDOUJUtu5p13lrF+/W4Pf5O6ozMIERGgffsWdOt2EgBnnNGK889v\nj5nRrdtJFBQUs3v3PoYPn8SqVTswMyoqAj9se/757WnevDEAXbq0Yt263bRt29yT36Mu6QxCRARI\nTPxxzo24OCMxMeGHx35/Jb///Wecd14mS5fewpQpv2TfPn+NbX/8v3Z8fNX69YECQkTkKOzeXU5G\nRjMAXnttocfVRIYCQkTkKPzud2dz332f0qvXS/XmDOFILJbmBsnJyXF5eXlelyEiMeif//wr/fqt\n5rTTTvSk/w8+KCQt7RZycnIi3reZzXfOHXPHYT+DMLO2ZvaZmX1rZsvM7M5g+8NmVmRmC4M//x7u\nWkSk4YqPT/D0f/5+vxEfrrnFwyQSdzH5gf9yzi0ws2bAfDObHlz2P865URGoQUQauObN27B9+3zP\n+t++3dG9e2zd2RT2Mwjn3Cbn3ILg4z3AciAj3P2KiNTUuXMPli2r9GTK7eLifezc2YRTTjkl4n3X\nRkQHqc0sE+gF/CvYdLuZLTazV8ysRSRrEZGGJTMzE8hixoz1EQ2JsrIKxo8vok+fS2PuElPEBqnN\nrCkwG3jcOTfBzE4CtgMOeAxo45y7PsR2NwI3ArRr1y573bp1EalXROqfsrIyxo17nr17l3H66dCi\nRQLx8eGZRa+iopKiIj+rVsXRu3cuF154sWeTBR7vIHVEAsLMfMAHwDTn3NMhlmcCHzjnuh7udXQX\nk4jUlnOOLVu2sHJlPnv27CAQCMcU3EajRkmkpqbTuXNnmjZtGoY+jqGa4wyIsA9SW1Vk/h1YXjMc\nzKyNc25T8OkVwNJw1yIiYmakpaWRlpYWkf4WL17Mo48+yksvvUTLli0j0mddicQYRH/g/wA/P+iW\n1j+b2RIzWwycB/zfCNQiIhJRXbp0ITMzkzPPPJPFixd7Xc4xCfsZhHPuCyDUhbcPw923iIjXEhIS\nGDVqFL179+b888/nhRde4Be/+IXXZR0VzeYqIhIB11xzDZ07dyY3N5dvvvmGxx57LOrvatJcTCIi\nEdK7d2/mzZvHV199xaWXXkpxcbHXJR2WAkJEJIJatWrFJ598QseOHenTpw/ffvut1yUdkgJCRCTC\nfD4ff/nLX7j//vs599xzmTRpktclhaSAEBHxyIgRI5g6dSq33347Dz/8MJWVVZMJzp49G7/ff4St\nw08BISLioT59+jBv3jxmzJjBFVdcQUlJCU888QTjx4/3ujQFhIiI19LS0pg5cybp6en069ePoUOH\n8tRTT3kysWBNCggRkShQXl7O8OHDueWWW7j//vvZtWsXM2bM8LQmfQ5CRCQKrF27lttuu43ly5fT\ntm1b1qxZw6233sqf3/mMp6atYGNxGekpSYwclMWQXpH5xgQFhIhIFOjRowd5eXmUlZWxYMECPvnk\nEyZ8OJ37JiyhrCIAQFFxGfdNWAIQkZBQQIiIRJGkpCT69+9P//79mZF0LkXFZQcsL6sI8NS0FREJ\nCI1BiIhEqY0HhcOR2uuaAkJEJEqlpyQdU3tdU0CIiESpkYOySPIdOKFfki+ekYOyItK/xiBERKJU\n9TiD7mISEZGfGNIrI2KBcDBdYhIRkZAUECIiEpICQkREQlJAiIhISAoIEREJSQEhIiIheR4QZjbY\nzFaY2Wozu9frekREpIqnAWFm8cDzwEVAF+CXZtbFy5pERKSK12cQfYDVzrm1zrn9wNvA5R7XJCIi\neB8QGcD6Gs83BNt+YGY3mlmemeVt27YtosWJiDRkXgfEETnnxjrncpxzOa1atfK6HBGRBsPrgCgC\n2tZ4fnKwTUREPOZ1QMwDOppZezNrBFwNvO9xTSIigsezuTrn/GZ2GzANiAdecc4t87ImERGp4vl0\n3865D4EPva5DREQO5PUlJhERiVIKCBERCUkBISIiISkgREQkJAWEiIiEpIAQEZGQFBAiIhKSAkJE\nREJSQIiISEgKCBERCUkBISIiISkgREQkJAWEiIiEpIAQEZGQFBAiIhKSAkJEREJSQIiISEgKCBER\nCUkBISIiISkgREQkJAWEiIiEFNaAMLOnzCzfzBab2UQzSwm2Z5pZmZktDP68GM46RETk2IX7DGI6\n0NU51x1YCdxXY9ka51zP4M9NYa5DRESOUVgDwjn3iXPOH3z6NXByOPsTEZG6E8kxiOuBj2o8bx+8\nvDTbzM451EZmdqOZ5ZlZ3rZt28JfpYiIAJBQ2xcwsxlAWohFDzjnJgfXeQDwA+OCyzYB7ZxzO8ws\nG5hkZmc450oOfhHn3FhgLEBOTo6rbb0iInJ0ah0QzrkLDrfczEYAlwDnO+dccJtyoDz4eL6ZrQE6\nAXm1rUdEROpGuO9iGgz8DrjMOVdao72VmcUHH58KdATWhrMWERE5NrU+gziC54BEYLqZAXwdvGNp\nAPComVUAlcBNzrmdYa5FRESOQVgDwjl32iHa3wPeC2ffIiJSO/oktYiIhKSAEBGRkBQQIiISkgJC\nRERCUkCIiEhICggREQlJASEiIiEpIEREJCQFhIiIhKSAEBGRkBQQIiISkgJCRERCUkCIiEhICggR\nEQlJASEiIiEpIEREJCQFhIiIhKSAEBGRkBQQIiISkgJCRERCSji4wcwMyADSAF/EKzrQPqDQObfD\n4zpERBqcAwKiUSPr3L0713fsSGr37rjkZKri4jh99x09d+0iKz6efQBt2jC/dWs2ABQU0G3XLrLM\nqMzI4F+tWlFUc9vKSiguhnnzsKwsW7lyJX/Lzs4+/mJEROSY/BAQjRpZx3POYeSf/kRJnz4U1sWL\nv/sup/p8zBkyhC9rtq9eTavJk2k7ahR/2bKFZm+9xbV33cXX8fG4g1/D78cmTeKUP/6Re/1+f12U\nJSIiR+GHMYhOnRj6wAOU9unD7nB3ungxWe3bszQxkUC7dhQ3bcrOpUvJCLVuQgJu6FA2//rXnFhe\nvjfcpYmISFACgJmlnHceHQcMYH1dd5CfT5/Ro+nRogUbc3OZlpLCvr17OSE9vepSE0ByMiXFxZwQ\navuPPiJ7+XKy9+whzrmtOOdqd91LRESOSvUlppYdOlCZkPDTSzxH8txzXLtvH00Pbu/bl0/PPZd5\nubnMNoPx4zlvwgQGXX89k4/l9S+6iPkXXcR8gA8/bPQHv9+Pz+f12LmISP1XHRAJjRsf3wvcdhtv\nHM16/fuz4J13uAagaVNKSkp+PGMoLeWElBRKjvQaCQlQUVGhgBARiYCwfg5iy5YfzywWLKDzCSew\nFaBbN1Z89x1dy8uJLywkZe9eWnbteuBdTCIi4q2ffA6iLk2ZwoXFxaSZQXIyxbm5TAHo2JFtmZks\ne+YZbo2Lo3LAAKaGuoNJRES8c1QBcc01nDN9Oj2bNeP71FR2d+nCpjlz6PTHP/LJsGFsXLWK5L59\nuXHnTp7Zvx/LzeWCxYvJ9PtJuPJK5j77bNUYwogRnD1zJmf4/ST078/yd9/lr199RcqQIfyqc2dO\nX7GCti1asGfOHN468UR0T6uIiIeOeInp3Xdp89lndF2+nBc//5xxa9eGvh212t1307tZM8oLC3k5\nP5+xkyaRPWcOKX/9Kx0KCmhZUMDLhYW8mJ9P+t/+xikA27fT8q67mLd5My80acK+p56iS139giIi\ncnyOeAYxbRqn9O9PfmoqFQB9+rDicOvPmUOHwkJOysioepMvKyNx/nxaTptGh8WL6dC2LTcBlJfT\naNkyTjzjDHa3aMGuK65gM0CXLmwsKCCl9r+aiIjUxnGPQcTHU1lZiQGUlPz4Os7Bgw/y4Z13sqbm\n+h9/TIdf/YrPqy83VfvqK1ISEghUP4+LwwUCmkRQRMRrR3wjHjyYdV9+SeedO0nYtIlGc+fSCaB1\na4q/+II2AGPG/HhJ6Gc/Y82rr3JmaWnVa8+cScutW/ENHsya99+n15YtNAL45huaLV9Ok/D8WiIi\nUltHPIMYOpRNEyawNCuLm5s14/sOHdgI8Pvf8+Xw4fxi0iSy+/ZlVfX6Tz/NgqFDSenQgd84h51w\nAt9/+ilv33EHa5YsIbVnT24ASExk/xtvMMHn091LIiLRyIJTV2Tddhsjn332x+kvDuWqqxiYnMz+\n1147cAK+SOjfv+Ufpk8vJDk5OdJdi4jELDOb75zLOdbtqi8xBcrL67iiMPD7ISEhrB/dEBGRoOp3\n213r1hFfWQlxRxiVGD+eWeEuKpTCQhrv3x+naTZERCIkDsA5t2PjRtbNnUtzrws6lBkzSC0vb6qZ\nXEVEIuSH84XVq5n0xBO0WLuWJC8LCmX2bE586SX2+3zNvC5FRKTB+OGCflmZW9i0qb0wbBjX5+SQ\n2rMncY0bUxkX581dRpWVWEkJcV99hVu8mJ3LljEqO9t3sxe1iIg0RAeM+O7d6+aa2cK8PE7z+WjT\npIl3ZxOVlbjvv2dvIEAhsM45V5mTc8yD8CIicpx+ckuQc24/8G3wx1Nm9irwd+fcd17XIiLS0ET7\nlBYTgHfN7HEza+R1MSIiDUlUB4RzbgrQE+gBfLlv3z6PKxIRaTiiOiAAnHNbgEuBv61YsYLnn38e\n5zQ7h4hIuFksvdl27drVJScnk5qayiuvvEJaWprXJYmIRL3aTrURExo3bsycOXM488wz6dmzJxMn\nTvS6JBGReiumAgLA5/PxyCOPMHHiREaOHMkNN9zAnj17vC5LRKTeibmAqHbWWWexcOFC4uLi6Nmz\nJ19++ePkst9//z2BQOAwW4uIyJHEbEAANG3alJdffpnRo0eTm5vL73//eyoqKhg9ejT33HOP1+WJ\niMS0sAaEmb1jZguDPwVmtjDYnmlmZTWWvVibfoYMGcLChQuZP38+Z599Nueddx6vvvoq69atq5tf\nRESkAQprQDjnhjnnejrnegLvUfXBt2prqpc5526qbV9paWlMnTqV6667jtzcXPr27ctDDz1U25cV\nEWmwIvLtO1Y1R/dVwM/D2c+wYcPYuHEjAwYM4Msvv2TLli2MGDGC3SmdeGraCjYWl5GeksTIQVkM\n6ZURzlJERGJepL6e7Rxgi3NuVY229sFLTruBB51zn4fa0MxuBG4EaNeu3WE7GTNmDEuXLmXlypVk\nZmYydepUxo7/gIWpF1JWUTVoXVRcxn0TlgAoJEREDqPWH5QzsxlAqE+sPeCcmxxcZwyw2jk3Ovg8\nEWjqnNthZtnAJOAM51zJ4frKyclxeXl5x1Rf/ydnUlRc9pP2jJQk5twb1hMaEZGocLwflKv1GYRz\n7oLDLTezBCAXyK6xTTlQHnw838zWAJ2AY3v3PwobQ4TD4dpFRKRKJG5zvQDId85tqG4ws1ZmFh98\nfCrQEVgbjs7TU0J/pcWh2kVEpEokAuJq4K2D2gYAi4NjEP8PuMk5tzMcnY8clEWSL/6AtiRfPCMH\nZYWjOxGReiPsg9TOuREh2t6j6rbXsKseiNZdTCIixyZSdzF5akivDAWCiMgxiumpNkREJHwUECIi\nEpICQkREQlJAiIhISAoIEREJqUHcxSTSUO3atYv8/OXs2rWFQKCizl8/Li6eJk1SOO20LDIyMqia\nl1PqCwWESD0UCASYMOEfFBTMpnNnR+vWCSQk1P0Fg0DAUVLiZ9IkMGvPf/zHnTRv3rzO+xFvKCBE\n6qFJk8YRCMzkt7/NJD4+/FeSzz/f8dVXG3j99VH85jcPkpiYGPY+Jfw0BiFSz5SUlLB69SyGDo1M\nOACYGWefnU5q6gZWrFgRkT4l/BQQIvXMihUr6NSpMiyXlI6kS5dE8vPnRrxfCQ8FhEg9U1y8g1at\nvPmnnZqaTHHxJk/6lrqngBCpZwKB/cTHe3M3UUJCHIGA35O+pe4pIEQakIKCYrp2feGAtry8jdxx\nx0ceVSTRTHcxiTRwOTnp5OSke12GRCEFhEgDtXbtLq68cjzXXNOV2bPX8cEH1/Dww7MoLNzN2rW7\nKCzczV139eOOO/oC8Nhjs/nnP5fQqlUybds2Jzu7DXfffbbHv4WEkwJCpAFasWI7V1/9Hq+9djm7\ndu1j9ux1PyzLz9/OZ58NZ8+e/WRlPcfNN+ewcOFm3ntvOYsW3URFRYDevceSnd3Gw99AIkFjECIN\nzLZtpVx++duMG5dLjx5pP1l+8cUdSUxMIDU1mdatm7Bly/fMmbOeyy/PonHjBJo1S+TSSzt5ULlE\nmgJCpIFp3jyRdu2a88UXhSGXJyb+eGEhPt7w+ysjVZpEGQWESAPTqFE8EycO4403FvHmm0uOapv+\n/dsyZcpK9u3zs3fvfj74YGWYq5RooIAQaYCaNGnEBx9cw//8z9eUlJQfcf0zz8zgssuy6N59DBdd\nNI5u3U6ieXPNt1TfmXOu9i9i9gvgYeB0oI9zLq/GsvuAG4AAcIdzblqwPRt4DUgCPgTudEcoJicn\nx+Xl5R1uFZEG7+OPp9C8+STOOqttnb7u3r37adq0EaWlFQwY8Cpjx15K794HDlRv3ryXiRObcPPN\nj9Rp31I7ZjbfOZdzrNvV1V1MS4Fc4KWDiuoCXA2cAaQDM8ysk3MuAIwB/hP4F1UBMRjQp3VEasnn\nS6Siou7HDW68cQrffruNffv8DB/e4yfhAFBREcDn05lFfVEnAeGcWw6E+rKQy4G3nXPlwHdmthro\nY2YFwAnOua+D270BDEEBIVJrqakn8e23df+6b7555RHXKSraQ2pqn7rvXDwR7jGIDGB9jecbgm0Z\nwccHt/+Emd1oZnlmlrdt27awFSpSX2RlZVFQkMDevfsj2m9lpWPJEj9dumRHtF8Jn6MOCDObYWZL\nQ/xcHs4CnXNjnXM5zrmcVq1ahbMrkXqhcePGnH32MN54Yz07dpRGpM+ysgomTSogMfFMOnToEJE+\nJfyO+hKTc+6C43j9IqDmSNnJwbai4OOD20WkDgwYcD4JCT5eeWUCJ5ywndatjYSE2t+QcrBAAEpK\noKgogc6d/41f/vIq4uPj67wf8Ua4p9p4H3jTzJ6mapC6IzDXORcwsxIz60fVIPW1wLNhrkWkwTAz\n+vc/l7POOofCwkKKi4sJBAJh6adJkyZkZmYCkJCg2Xvqkzr52zSzK6h6g28FTDWzhc65Qc65ZWY2\nHvgW8AO3Bu9gAriFH29z/QgNUIvUubi4uB/evMPt+uuvx+/3M3bsWBo3bhyRPiW86uRzEJGiz0GI\nRK/S0lKuv/561q5dy8SJE8nICHnfiXjgeD8HoU9Si0idSE5O5q233uKKK66gb9++fP31116XJLWk\ngBCROmNm3HfffYwZM4bLLruM1157zeuSpBYUECJS5y699FJmzZrF448/zl133YXfr++pjkUKCBEJ\niy5dujB37lyWL1/O4MGD2bFjh9clyTFSQIhI2LRo0YKpU6fSo0cP+vbty7Jly7wuSY6BAkJEwioh\nIYHRo0fz0EMPMXDgQCZPngzAwoULmTJlisfVyeEoIEQkIq699lqmTp3KrbfeymOPPUYgEOCWW26h\noqLC69LkEBQQIhIxffr0Ye7cuUydOpUnn3yS9u3bM378eK/LkkNQQIhIRE2ePJlbbrkF5xzr16/n\nT3/6E7H0gd2GRBOniEhE+Xw+Jk6cyFdffUVpaSkFBQX83z8+yzxfVzYWl5GeksTIQVkM6aVPYntN\nU22IiCeccxQUFPDgqBf5cn8mrmW7H5Yl+eL5U243hUQd0VQbIhJTzIz27dtT0HbQAeEAUFYR4Klp\nKzyqTKpUPjpNAAAE0UlEQVQpIETEUxuLy46pXSJHASEinkpPSTqmdokcBYSIeGrkoCySfAd+C12S\nL56Rg7I8qkiq6S4mEfFU9UD0U9NW6C6mKKOAEBHPDemVoUCIQrrEJCIiISkgREQkJAWEiIiEpIAQ\nEZGQFBAiIhJSTM3FZGbbgHWHWSUV2B6hcupSrNYNsVt7rNYNsVu76o686tpPcc61OtaNYyogjsTM\n8o5nQiqvxWrdELu1x2rdELu1q+7Iq23tusQkIiIhKSBERCSk+hYQY70u4DjFat0Qu7XHat0Qu7Wr\n7sirVe31agxCRETqTn07gxARkTqigBARkZBiJiDM7BUz22pmS2u0PWxmRWa2MPjz7zWW3Wdmq81s\nhZkN8qbqH2oJVfs7NeouMLOFwfZMMyursexFD+tua2afmdm3ZrbMzO4Mtp9oZtPNbFXwzxY1tvF8\nvx+m7qfMLN/MFpvZRDNLCbbHwj6P6mP9MHXHwnHe2MzmmtmiYO2PBNuj/Tg/VN11d5w752LiBxgA\n9AaW1mh7GLg7xLpdgEVAItAeWAPER1PtBy0fDTwUfJx5qPU8qLsN0Dv4uBmwMrhv/wzcG2y/F/jv\naNrvh6n734CEYPt/16g7FvZ5VB/rh6r7oHWi9Tg3oGnwsQ/4F9AvBo7zQ9VdZ8d5zJxBOOf+F9h5\nlKtfDrztnCt3zn0HrAb6hK24Izhc7WZmwFXAWxEt6ig45zY55xYEH+8BlgMZVO3f14OrvQ4MCT6O\niv1+qLqdc5845/zB1b4GTo50bUdymH1+KFG9z6uXR/lx7pxze4NPfcEfR/Qf5yHrrsvjPGYC4jBu\nD55KvVLjFDADWF9jnQ0c/h+Zl84BtjjnVtVoax88BZxtZud4VVhNZpYJ9KLqfyknOec2BRdtBk4K\nPo66/X5Q3TVdD3xU43m073OIkWP9EPs8qo9zM4sPXv7aCkx3zsXEcX6Iumuq1XEe6wExBjgV6Als\nouoUNtb8kgP/V7UJaOec6wn8FnjTzE7wpLIgM2sKvAfc5ZwrqbnMVZ27RuW90oeq28weAPzAuGBT\nLOzzmDjWD3OsRPVx7pwLBGs5GehjZl0PWh6Vx/nh6q6L4zymA8I5tyW4gyqBl/nxNK8IaFtj1ZOD\nbVHFzBKAXOCd6rbgaeuO4OP5VF3f7ORNhWBmPqr+wY9zzk0INm8xszbB5W2o+t8LRNF+P0TdmNkI\n4BLgV8F/9DGxz2PhWD/MPo/647yac64Y+AwYTAwc59UOqrvOjvOYDojqv7ygK4Dqu4TeB642s0Qz\naw90BOZGur6jcAGQ75zbUN1gZq3MLD74+FSqal/rRXHB68Z/B5Y7556useh9YHjw8XBgco12z/f7\noeo2s8HA74DLnHOlNdqjfp9H+7F+mGMFov84b1XjTp8k4EIgn+g/zkPWXafHeV2Oqofzh6rT001A\nBVXX/G4A/gEsARZT9ZfWpsb6D1CVkCuAi6Kt9mD7a8BNB617JbAMWAgsAC71sO6fUXVavThYz0Lg\n34GWwKfAKmAGcGI07ffD1L2aqmvH1W0vxtA+j+pj/VB1x8hx3h34Jlj7Un680yraj/ND1V1nx7mm\n2hARkZBi+hKTiIiEjwJCRERCUkCIiEhICggREQlJASEiIiEpIEREJCQFhIiIhPT/AUh2tjp8Brxt\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feaf55de9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot\n",
    "import pylab\n",
    "\n",
    "x = [1,2,3,4]\n",
    "y = [3,4,8,6]\n",
    "\n",
    "matplotlib.pyplot.scatter(dimReducVecs_gender[:,0], dimReducVecs_gender[:,1])\n",
    "labels = [\"man\", \"women\", \"king\", \"queen\"]\n",
    "for label, x, y in zip(labels, dimReducVecs_gender[:, 0], dimReducVecs_gender[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "    \n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([lookupWordVec(word = \"spain\"), lookupWordVec(word = \"madrid\"),\n",
    "              lookupWordVec(word = \"italy\"), lookupWordVec(word = \"rome\"),\n",
    "             lookupWordVec(word = \"japan\"), lookupWordVec(word = \"tokyo\")])\n",
    "dimReducVecs_caps = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEMCAYAAADNtWEcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7feaf4f40518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matplotlib.pyplot.scatter(dimReducVecs_caps[:,0], dimReducVecs_caps[:,1])\n",
    "labels = [\"spain\", \"madrid\", \"italy\", \"rome\", \"japan\", \"tokyo\"]\n",
    "for label, x, y in zip(labels, dimReducVecs_caps[:, 0], dimReducVecs_caps[:, 1]):\n",
    "    plt.annotate(\n",
    "        label,\n",
    "        xy=(x, y), xytext=(-20, 20),\n",
    "        textcoords='offset points', ha='right', va='bottom',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "        arrowprops=dict(arrowstyle = '->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "    \n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">TODO: </span> describe your observation of these word vectors\n",
    "\n",
    "Answer: **[Especially with the man-woman king-queen vectors we see some decent patterns. Word vectors should cluster next to words where they appear in the same context. As we can see this is more or less true and we see that certain vectors appear parallel to each other. We don't see perfect clustering of similar words, but this could be due to TSNE being difficult to interpret and variable, or to how long we trained our model for. Especially when it comes to the capital and country word vectors we don't see as clear a pattern as we do with the king-queen and man-women vectors. This could be due to how often these words showed up in the text and thus how well they were trained.]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
