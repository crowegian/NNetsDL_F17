{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM4040 Assignment 1, Task 4: Questions\n",
    "\n",
    "1) What is the difference between the SVM method and a neural network, assuming that both work with the same number of training samples N?\n",
    "\n",
    "   Your answer: **The SVM doesn't try to predict probabilities, but rather tries to find a line that separates the data into its different classes. Even though both methods can transform the data (the SVM using the kernel trick and the NNet using repeated affine transformations followed by non-linearities) the NNet can still output class probabilities. The most important difference between the two is that the SVM still requires manual curation of features, while the advantage of NNets is that they can do feature extraction by themselves. The big thing about NNets is their representation learning which occures within the hidden layers.**\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "2) Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: **In general ReLUs are preferred over other activation methods because of the vanishing gradient problem. ReLU does not have the vanishing gradient problem because it does not saturate like Sigmoid activation functions do.**\n",
    "   \n",
    "\n",
    "3) Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  the hyperparameters, which stategies you did you use to improve the network, show the results of intermediate and final steps.\n",
    "\n",
    "   Your answer: **My best model achieved an accuracy on the test set of 0.505. I started with the default parameters from the assignment and tweaked the model from there. I found that the batch size played a large role in performance so I played with this first (actually learnign rate technically played the largerst part because the model even learning was so dependent on this value) and found a good value before moving on. I then played aroudn with the number of units in the hidden layer. This one is tricker because more hidden units means the model can learn more, but if the data is too small the model won't generalize well. Regularization is important as it keeps the model from overtraining on the training data and allows it to generalize better and this was the paramter I tuned last.**\n",
    "   \n",
    "   **The problems with this kind of tuning is that I miss out on a lot of parameter combinations which could do well because of my greedy search approach. But due to resource constraints I chose to perform my search this way. Another issue is the variability between runs. Sometimes the same parameters can perform fairly dfiferently (1-2%) between runs. If I were to do this properly I would perform grid seach and average for each set of parameters. I did implement early stopping so that if model performance doesn't increase above the best validation set accuracy for two turns then training is stopped. The best model is the one which achieved the highest validation set accuracy and is saved whenever a new best performance is found.\n",
    "   hidden_dim = 100\n",
    "   reg = 0.3\n",
    "   batch_size = 256\n",
    "   lr = 1e-4\n",
    "   **\n",
    "   \n",
    "\n",
    "4) **Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation** if you want to use it to find a best set of hyperparameter of the **Linear SVM classification** problem.\n",
    "\n",
    "   Your answer: **In k-fold cv you split your data randomly into k different sections. You then train on k-1 folds of the data, and test on the left out fold. You repeat this using all folds as a test set once, and then average your performance (accuracy for example) over all folds. This gives you a better estimation of your testing error since you train multiple models with the same parameters on the data and average their performance which reduces the bias of your estimation. You can perform k-fold cv for the same model with different sets of parameters to perform grid search and see which set of parameters has the best estimated testing error.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
