{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECBM E4040 - Assignment 2- Task 5: Kaggle Open-ended Competition\n",
    "\n",
    "Kaggle is a platform for predictive modelling and analytics competitions in which companies and researchers post data and statisticians and data miners compete to produce the best models for predicting and describing the data.\n",
    "\n",
    "If you don't have a Kaggle account, feel free to join at [www.kaggle.com](https://www.kaggle.com). To let the TAs do the grading more conveniently, please use Lionmail to join Kaggle and use UNI as your username.\n",
    "\n",
    "Visit the website for this competition to join: \n",
    "[https://www.kaggle.com/t/8dd419892b1c49a3afb0cea385a7e677](https://www.kaggle.com/t/8dd419892b1c49a3afb0cea385a7e677)\n",
    "\n",
    "Details about this in-class competition is shown on the website above. Please read carefully.\n",
    "\n",
    "<span style=\"color:red\">__TODO__:</span>\n",
    "1. Train a custom model for the bottle dataset classification problem. You are free to use any methods taught in the class or found by yourself on the Internet (ALWAYS provide reference to the source). General training methods include:\n",
    "    * Dropout\n",
    "    * Batch normalization\n",
    "    * Early stopping\n",
    "    * l1-norm & l2-norm penalization\n",
    "2. You'll be given the test set to generate your predictions (70% public + 30% private, but you don't know which ones are public/private). Achieve 70% accuracy on the public test set. The accuracy will be shown on the public leaderboard once you submit your prediction .csv file. \n",
    "3. (A) Report your results on the Kaggle, for comparison with other students' optimization results (you should do this several times). (C) Save your best model, using BitBucket, at the same time when you (B) submit the homework files into Courseworks. See instructions below. \n",
    "\n",
    "__Hint__: You can start from what you implemented in task 4. Another classic classification model named 'VGG16' can also be easily implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW Submission Details:\n",
    "There are three components to reporting the results of this task: \n",
    "\n",
    "**(A) Submission (possible several) of the .csv prediction file throught the Kaggle platform;**. You should start doing this VARY early, so that students can compare their work as they are making progress with model optimization.\n",
    "\n",
    "**(B) Editing and submitting the content of this Jupyter notebook, through Courseworks; **\n",
    "(i) The code for your CNN model and for the training function. The code should be stored in __./ecbm4040/neuralnets/kaggle.py__;\n",
    "(ii) Print out your training process and accuracy __within this notebook__;\n",
    "\n",
    "**(C) Submitting your best CNN model through instructor-owned private BitBucket repo.**\n",
    "\n",
    "**Description of (C):** \n",
    "For this task, you will be utilizing bitbucket to save your model for submission. Bitbucket provides Git code managment. For those who are not familiar with git operations, please check [Learn Git with Bitbucket Cloud](https://www.atlassian.com/git/tutorials/learn-git-with-bitbucket-cloud) as reference.\n",
    "**TAs will create a private Bitbucket repository for each student, with the write access. This repo will be owned by the instructors. Make sure to properly submit your model to that exact repository (submissions to your own private repository will not count)** Students need to populate the following file to provide instructors with bitbucket account information: https://docs.google.com/spreadsheets/d/1_7cZjyr34I2y-AD_0N5UaJ3ZnqdhYcvrdoTsYvOSd-g/edit#gid=0.\n",
    "\n",
    "<span style=\"color:red\">__Submission content:__ :</span>\n",
    "(i) Upload your best model with all the data output (for example, __MODEL.data-00000-of-00001, MODEL.meta, MODEL.index__) into the  BitBucket. Store your model in the folder named \"__KaggleModel__\" within the BitBucket repository. \n",
    "Remember to delete any intermediate results, **we only want your best model. Do not upload any data files**. The instructors will rerun the uploaded best model and verify against the score which you reported on the Kaggle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# import tensorflow as tf\n",
    "import os.path\n",
    "import numpy as np\n",
    "from ecbm4040.neuralnets.kaggle import my_training\n",
    "import glob\n",
    "import matplotlib.image as img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "def loadImages():\n",
    "    imgReshapeSize = 32\n",
    "    X_all = np.zeros((15000, imgReshapeSize, imgReshapeSize, 3))\n",
    "    y_all = np.zeros(15000)\n",
    "    x_class = np.zeros((3000, imgReshapeSize, imgReshapeSize, 3))\n",
    "    saveFileTrain = \"data/kaggle/X_train.npy\"\n",
    "    if False:#(os.path.exists(saveFileTrain)):\n",
    "        X_all = np.load(saveFileTrain)\n",
    "    else:\n",
    "        for imageClass in np.arange(0,5):\n",
    "            print(\"Reading all images in class {}\".format(imageClass))\n",
    "            classImagePaths = glob.glob(\"data/kaggle/train_128/{}/*.png\".format(imageClass))\n",
    "#             scipy.misc.imresize(original_image, (i_height, i_width))\n",
    "            x_class = np.array([scipy.misc.imresize(img.imread((fname)), (imgReshapeSize, imgReshapeSize)) for fname in classImagePaths])\n",
    "            print(\"{} images found\".format(len(classImagePaths)))\n",
    "            print(\"*\"*100)\n",
    "            X_all[np.arange(imageClass*3000, (imageClass + 1)*3000), :] = x_class\n",
    "            y_all[np.arange(imageClass*3000, (imageClass + 1)*3000)] = np.ones(3000)*imageClass        \n",
    "        #     return(X_all, y_all)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_all, y_all, test_size = 0.25, random_state = 23)\n",
    "#     np.save(saveFileTrain, X_all)img.imread((fname))\n",
    "    X_all = None\n",
    "    y_all = None\n",
    "    print(\"Reading test Data\")\n",
    "    X_test = np.zeros((3500, imgReshapeSize, imgReshapeSize, 3))\n",
    "    saveFileTest = \"data/kaggle/X_train.npy\"\n",
    "    if False:#(os.path.exists(saveFileTest)):\n",
    "        X_test = np.load(saveFileTest)\n",
    "    else:\n",
    "        extension = \".png\"\n",
    "        path_to_image_folder = \"data/kaggle/test_128/\"#Wherever you have your images\n",
    "        num_test_samples = 3500 #Ideally you could count the elements in the folder\n",
    "        xTestFiles = [path_to_image_folder + str(idx)+extension for idx in range(num_test_samples)]\n",
    "        X_test = np.array([scipy.misc.imresize(img.imread((fname)), (imgReshapeSize, imgReshapeSize)) for fname in xTestFiles])\n",
    "    print(\"X_train shape {}\\n X_val shape {}\\n X_test shape {}\".format(X_train.shape, X_val.shape, X_test.shape))\n",
    "    print(\"y_train shape {}\\n y_val shape {}\".format(y_train.shape, y_val.shape))\n",
    "#     np.save(saveFileTest, X_test)\n",
    "    return(X_train, X_val, X_test, y_train, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading all images in class 0\n",
      "3000 images found\n",
      "****************************************************************************************************\n",
      "Reading all images in class 1\n",
      "3000 images found\n",
      "****************************************************************************************************\n",
      "Reading all images in class 2\n",
      "3000 images found\n",
      "****************************************************************************************************\n",
      "Reading all images in class 3\n",
      "3000 images found\n",
      "****************************************************************************************************\n",
      "Reading all images in class 4\n",
      "3000 images found\n",
      "****************************************************************************************************\n",
      "Reading test Data\n",
      "X_train shape (11250, 32, 32, 3)\n",
      " X_val shape (3750, 32, 32, 3)\n",
      " X_test shape (3500, 32, 32, 3)\n",
      "y_train shape (11250,)\n",
      " y_val shape (3750,)\n"
     ]
    }
   ],
   "source": [
    "# X_all, y_all = loadImages()\n",
    "X_train, X_val, X_test, y_train, y_val = loadImages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building my LeNet. Parameters: \n",
      "conv_featmap=[[20, 12, 12]]\n",
      "fc_units=[500, 250, 100, 100]\n",
      "conv_kernel_size=[[5, 5, 5]]\n",
      "pooling_size=[2]\n",
      "l2_norm=0.01\n",
      "seed=235\n",
      "learning_rate=0.01\n",
      "pooling layer shape (?, 16, 16, 12) at pool 0\n",
      "Input shape for fully connected layer: (?, 3072)\n",
      "number of batches for training: 703\n",
      "epoch 1\n",
      "Best validation accuracy! iteration:100 accuracy: 28.08%\n",
      "Best validation accuracy! iteration:200 accuracy: 35.81333333333333%\n",
      "Best validation accuracy! iteration:300 accuracy: 42.88%\n",
      "Best validation accuracy! iteration:400 accuracy: 49.78666666666667%\n",
      "Best validation accuracy! iteration:500 accuracy: 51.626666666666665%\n",
      "Best validation accuracy! iteration:600 accuracy: 53.306666666666665%\n",
      "Best validation accuracy! iteration:700 accuracy: 54.693333333333335%\n",
      "epoch time 12.362178325653076\n",
      "epoch 2\n",
      "Best validation accuracy! iteration:800 accuracy: 56.08%\n",
      "Best validation accuracy! iteration:900 accuracy: 56.346666666666664%\n",
      "Best validation accuracy! iteration:1000 accuracy: 56.586666666666666%\n",
      "Best validation accuracy! iteration:1100 accuracy: 56.88%\n",
      "Best validation accuracy! iteration:1200 accuracy: 57.84%\n",
      "Best validation accuracy! iteration:1300 accuracy: 58.64%\n",
      "epoch time 12.07947325706482\n",
      "epoch 3\n",
      "Best validation accuracy! iteration:1500 accuracy: 58.96%\n",
      "Best validation accuracy! iteration:1600 accuracy: 59.57333333333333%\n",
      "Best validation accuracy! iteration:1800 accuracy: 60.08%\n",
      "Best validation accuracy! iteration:1900 accuracy: 60.4%\n",
      "Best validation accuracy! iteration:2000 accuracy: 60.53333333333333%\n",
      "Best validation accuracy! iteration:2100 accuracy: 60.906666666666666%\n",
      "epoch time 12.101238250732422\n",
      "epoch 4\n",
      "Best validation accuracy! iteration:2200 accuracy: 61.70666666666666%\n",
      "Best validation accuracy! iteration:2400 accuracy: 61.973333333333336%\n",
      "Best validation accuracy! iteration:2500 accuracy: 62.24%\n",
      "Best validation accuracy! iteration:2700 accuracy: 62.42666666666667%\n",
      "Best validation accuracy! iteration:2800 accuracy: 62.586666666666666%\n",
      "epoch time 12.01680040359497\n",
      "epoch 5\n",
      "Best validation accuracy! iteration:2900 accuracy: 62.74666666666667%\n",
      "Best validation accuracy! iteration:3000 accuracy: 63.306666666666665%\n",
      "Best validation accuracy! iteration:3200 accuracy: 63.52%\n",
      "Best validation accuracy! iteration:3300 accuracy: 64.13333333333333%\n",
      "Best validation accuracy! iteration:3500 accuracy: 64.18666666666667%\n",
      "epoch time 11.738017082214355\n",
      "epoch 6\n",
      "Best validation accuracy! iteration:3600 accuracy: 64.82666666666667%\n",
      "Best validation accuracy! iteration:3700 accuracy: 64.90666666666667%\n",
      "Best validation accuracy! iteration:3800 accuracy: 65.17333333333333%\n",
      "Best validation accuracy! iteration:3900 accuracy: 65.52000000000001%\n",
      "Best validation accuracy! iteration:4000 accuracy: 65.62666666666667%\n",
      "Best validation accuracy! iteration:4100 accuracy: 65.70666666666666%\n",
      "Best validation accuracy! iteration:4200 accuracy: 66.42666666666668%\n",
      "epoch time 12.28897738456726\n",
      "epoch 7\n",
      "Best validation accuracy! iteration:4300 accuracy: 66.8%\n",
      "Best validation accuracy! iteration:4400 accuracy: 66.98666666666666%\n",
      "Best validation accuracy! iteration:4600 accuracy: 67.52000000000001%\n",
      "Best validation accuracy! iteration:4800 accuracy: 67.70666666666666%\n",
      "Best validation accuracy! iteration:4900 accuracy: 68.10666666666667%\n",
      "epoch time 11.86469292640686\n",
      "epoch 8\n",
      "Best validation accuracy! iteration:5100 accuracy: 68.16%\n",
      "Best validation accuracy! iteration:5200 accuracy: 68.21333333333334%\n",
      "Best validation accuracy! iteration:5300 accuracy: 68.29333333333334%\n",
      "Best validation accuracy! iteration:5400 accuracy: 68.34666666666666%\n",
      "Best validation accuracy! iteration:5500 accuracy: 68.88%\n",
      "epoch time 11.737531661987305\n",
      "epoch 9\n",
      "Best validation accuracy! iteration:5700 accuracy: 69.49333333333334%\n",
      "Best validation accuracy! iteration:5800 accuracy: 69.68%\n",
      "Best validation accuracy! iteration:5900 accuracy: 69.89333333333333%\n",
      "Best validation accuracy! iteration:6000 accuracy: 70.18666666666667%\n",
      "Best validation accuracy! iteration:6300 accuracy: 70.45333333333333%\n",
      "epoch time 11.823591709136963\n",
      "epoch 10\n",
      "Best validation accuracy! iteration:6400 accuracy: 70.8%\n",
      "Best validation accuracy! iteration:6500 accuracy: 71.06666666666666%\n",
      "Best validation accuracy! iteration:6800 accuracy: 71.2%\n",
      "Best validation accuracy! iteration:7000 accuracy: 71.28%\n",
      "epoch time 11.582392930984497\n",
      "epoch 11\n",
      "Best validation accuracy! iteration:7200 accuracy: 72.24%\n",
      "epoch time 10.799255132675171\n",
      "epoch 12\n",
      "Best validation accuracy! iteration:7800 accuracy: 72.29333333333334%\n",
      "Best validation accuracy! iteration:7900 accuracy: 72.8%\n",
      "Best validation accuracy! iteration:8400 accuracy: 72.93333333333334%\n",
      "epoch time 11.30746579170227\n",
      "epoch 13\n",
      "Best validation accuracy! iteration:8700 accuracy: 73.36%\n",
      "Best validation accuracy! iteration:9000 accuracy: 73.86666666666667%\n",
      "epoch time 11.03003716468811\n",
      "epoch 14\n",
      "Best validation accuracy! iteration:9200 accuracy: 74.26666666666667%\n",
      "Best validation accuracy! iteration:9700 accuracy: 74.50666666666666%\n",
      "Best validation accuracy! iteration:9800 accuracy: 74.77333333333334%\n",
      "epoch time 11.444005012512207\n",
      "epoch 15\n",
      "Best validation accuracy! iteration:10000 accuracy: 74.8%\n",
      "Best validation accuracy! iteration:10100 accuracy: 75.06666666666666%\n",
      "Best validation accuracy! iteration:10200 accuracy: 75.09333333333333%\n",
      "Best validation accuracy! iteration:10500 accuracy: 75.17333333333333%\n",
      "epoch time 11.6626296043396\n",
      "epoch 16\n",
      "Best validation accuracy! iteration:10600 accuracy: 75.70666666666666%\n",
      "Best validation accuracy! iteration:10700 accuracy: 75.73333333333333%\n",
      "Best validation accuracy! iteration:11100 accuracy: 76.24%\n",
      "epoch time 11.365917444229126\n",
      "epoch 17\n",
      "Best validation accuracy! iteration:11900 accuracy: 76.4%\n",
      "epoch time 10.875877380371094\n",
      "epoch 18\n",
      "Best validation accuracy! iteration:12000 accuracy: 76.69333333333333%\n",
      "Best validation accuracy! iteration:12500 accuracy: 76.74666666666667%\n",
      "epoch time 11.17901086807251\n",
      "epoch 19\n",
      "Best validation accuracy! iteration:12800 accuracy: 77.30666666666667%\n",
      "Best validation accuracy! iteration:13200 accuracy: 77.54666666666667%\n",
      "epoch time 11.094757795333862\n",
      "epoch 20\n",
      "Best validation accuracy! iteration:13900 accuracy: 77.81333333333333%\n",
      "Best validation accuracy! iteration:14000 accuracy: 77.84%\n",
      "epoch time 11.00114631652832\n",
      "epoch 21\n",
      "Best validation accuracy! iteration:14300 accuracy: 78.02666666666667%\n",
      "Best validation accuracy! iteration:14600 accuracy: 78.18666666666667%\n",
      "epoch time 11.092967987060547\n",
      "epoch 22\n",
      "Best validation accuracy! iteration:14900 accuracy: 78.8%\n",
      "Best validation accuracy! iteration:15400 accuracy: 78.82666666666667%\n",
      "epoch time 11.131025552749634\n",
      "epoch 23\n",
      "Best validation accuracy! iteration:15600 accuracy: 79.17333333333333%\n",
      "epoch time 10.817070722579956\n",
      "epoch 24\n",
      "Best validation accuracy! iteration:16700 accuracy: 79.46666666666667%\n",
      "epoch time 10.80118465423584\n",
      "epoch 25\n",
      "Best validation accuracy! iteration:17000 accuracy: 79.73333333333333%\n",
      "Best validation accuracy! iteration:17500 accuracy: 80.10666666666667%\n",
      "epoch time 11.030274868011475\n",
      "epoch 26\n",
      "epoch time 10.688258409500122\n",
      "epoch 27\n",
      "Best validation accuracy! iteration:18500 accuracy: 80.16%\n",
      "Best validation accuracy! iteration:18900 accuracy: 80.45333333333333%\n",
      "epoch time 11.322481393814087\n",
      "epoch 28\n",
      "Best validation accuracy! iteration:19400 accuracy: 80.50666666666666%\n",
      "Best validation accuracy! iteration:19600 accuracy: 80.53333333333333%\n",
      "epoch time 11.075633525848389\n",
      "epoch 29\n",
      "Best validation accuracy! iteration:19800 accuracy: 80.66666666666667%\n",
      "Best validation accuracy! iteration:19900 accuracy: 80.88%\n",
      "epoch time 11.236160278320312\n",
      "epoch 30\n",
      "Best validation accuracy! iteration:20400 accuracy: 81.01333333333334%\n",
      "Best validation accuracy! iteration:20600 accuracy: 81.03999999999999%\n",
      "Best validation accuracy! iteration:20800 accuracy: 81.06666666666666%\n",
      "epoch time 11.391765117645264\n",
      "epoch 31\n",
      "Best validation accuracy! iteration:21100 accuracy: 81.17333333333333%\n",
      "Best validation accuracy! iteration:21200 accuracy: 81.6%\n",
      "epoch time 11.069588661193848\n",
      "epoch 32\n",
      "Best validation accuracy! iteration:21900 accuracy: 81.62666666666667%\n",
      "epoch time 10.807840347290039\n",
      "epoch 33\n",
      "Best validation accuracy! iteration:23000 accuracy: 81.78666666666666%\n",
      "epoch time 10.881852388381958\n",
      "epoch 34\n",
      "Best validation accuracy! iteration:23300 accuracy: 81.81333333333333%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch time 11.09414267539978\n",
      "epoch 35\n",
      "Best validation accuracy! iteration:24300 accuracy: 81.97333333333333%\n",
      "Best validation accuracy! iteration:24600 accuracy: 82.08%\n",
      "epoch time 11.079782247543335\n",
      "epoch 36\n",
      "epoch time 10.55835509300232\n",
      "epoch 37\n",
      "Best validation accuracy! iteration:25400 accuracy: 82.61333333333333%\n",
      "epoch time 10.856332540512085\n",
      "epoch 38\n",
      "epoch time 10.536652088165283\n",
      "epoch 39\n",
      "epoch time 10.557482957839966\n",
      "epoch 40\n",
      "Best validation accuracy! iteration:27600 accuracy: 82.90666666666667%\n",
      "epoch time 10.869893074035645\n",
      "epoch 41\n",
      "Best validation accuracy! iteration:28300 accuracy: 83.09333333333333%\n",
      "epoch time 10.845669984817505\n",
      "epoch 42\n",
      "Best validation accuracy! iteration:29000 accuracy: 83.22666666666666%\n",
      "Best validation accuracy! iteration:29100 accuracy: 83.44%\n",
      "epoch time 11.104509353637695\n",
      "epoch 43\n",
      "epoch time 10.590859413146973\n",
      "epoch 44\n",
      "epoch time 10.564778327941895\n",
      "epoch 45\n",
      "Best validation accuracy! iteration:31100 accuracy: 83.57333333333334%\n",
      "epoch time 10.938634872436523\n",
      "epoch 46\n",
      "epoch time 10.620395183563232\n",
      "epoch 47\n",
      "epoch time 10.563333511352539\n",
      "epoch 48\n",
      "epoch time 10.59222960472107\n",
      "epoch 49\n",
      "Best validation accuracy! iteration:34000 accuracy: 83.76%\n",
      "epoch time 10.794925689697266\n",
      "epoch 50\n",
      "Best validation accuracy! iteration:34700 accuracy: 83.92%\n",
      "epoch time 10.851450204849243\n",
      "Traning ends. The best valid accuracy is 83.92. Model named kaggleModel_1509632835.\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "# from ecbm4040.neuralnets.kaggle import my_training\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[20, 12, 12]]\n",
    "fc_units=[500, 250, 100, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=16,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building my LeNet. Parameters: \n",
      "conv_featmap=[[20, 12, 12]]\n",
      "fc_units=[500, 250, 100, 100]\n",
      "conv_kernel_size=[[4, 4, 4]]\n",
      "pooling_size=[2]\n",
      "l2_norm=0.01\n",
      "seed=235\n",
      "learning_rate=0.01\n",
      "pooling layer shape (?, 16, 16, 12) at pool 0\n",
      "Input shape for fully connected layer: (?, 3072)\n",
      "number of batches for training: 703\n",
      "epoch 1\n",
      "Best validation accuracy! iteration:100 accuracy: 27.28%\n",
      "Best validation accuracy! iteration:200 accuracy: 36.53333333333333%\n",
      "Best validation accuracy! iteration:300 accuracy: 41.28%\n",
      "Best validation accuracy! iteration:400 accuracy: 47.733333333333334%\n",
      "Best validation accuracy! iteration:500 accuracy: 51.44%\n",
      "Best validation accuracy! iteration:600 accuracy: 53.06666666666667%\n",
      "Best validation accuracy! iteration:700 accuracy: 54.96%\n",
      "epoch time 13.051982879638672\n",
      "epoch 2\n",
      "Best validation accuracy! iteration:800 accuracy: 55.946666666666665%\n",
      "Best validation accuracy! iteration:900 accuracy: 56.77333333333333%\n",
      "Best validation accuracy! iteration:1000 accuracy: 57.86666666666667%\n",
      "Best validation accuracy! iteration:1100 accuracy: 58.53333333333333%\n",
      "Best validation accuracy! iteration:1200 accuracy: 58.906666666666666%\n",
      "Best validation accuracy! iteration:1300 accuracy: 59.44%\n",
      "Best validation accuracy! iteration:1400 accuracy: 60.29333333333334%\n",
      "epoch time 12.135294914245605\n",
      "epoch 3\n",
      "Best validation accuracy! iteration:1700 accuracy: 60.4%\n",
      "Best validation accuracy! iteration:1800 accuracy: 61.46666666666667%\n",
      "Best validation accuracy! iteration:1900 accuracy: 61.92%\n",
      "Best validation accuracy! iteration:2100 accuracy: 62.82666666666667%\n",
      "epoch time 11.352963924407959\n",
      "epoch 4\n",
      "Best validation accuracy! iteration:2400 accuracy: 63.12%\n",
      "Best validation accuracy! iteration:2600 accuracy: 63.17333333333333%\n",
      "Best validation accuracy! iteration:2700 accuracy: 63.54666666666667%\n",
      "Best validation accuracy! iteration:2800 accuracy: 64.32%\n",
      "epoch time 11.442011594772339\n",
      "epoch 5\n",
      "Best validation accuracy! iteration:3200 accuracy: 65.12%\n",
      "Best validation accuracy! iteration:3500 accuracy: 65.25333333333333%\n",
      "epoch time 10.924480676651001\n",
      "epoch 6\n",
      "Best validation accuracy! iteration:3600 accuracy: 66.10666666666667%\n",
      "Best validation accuracy! iteration:3900 accuracy: 66.24000000000001%\n",
      "Best validation accuracy! iteration:4200 accuracy: 66.56%\n",
      "epoch time 11.108957767486572\n",
      "epoch 7\n",
      "Best validation accuracy! iteration:4500 accuracy: 67.17333333333333%\n",
      "Best validation accuracy! iteration:4900 accuracy: 67.33333333333334%\n",
      "epoch time 10.973151445388794\n",
      "epoch 8\n",
      "Best validation accuracy! iteration:5000 accuracy: 67.38666666666666%\n",
      "Best validation accuracy! iteration:5100 accuracy: 68.32%\n",
      "Best validation accuracy! iteration:5300 accuracy: 68.50666666666666%\n",
      "epoch time 11.429742813110352\n",
      "epoch 9\n",
      "Best validation accuracy! iteration:5700 accuracy: 68.58666666666667%\n",
      "Best validation accuracy! iteration:5800 accuracy: 69.2%\n",
      "Best validation accuracy! iteration:6000 accuracy: 69.30666666666667%\n",
      "Best validation accuracy! iteration:6100 accuracy: 69.52%\n",
      "Best validation accuracy! iteration:6300 accuracy: 69.78666666666666%\n",
      "epoch time 11.617155075073242\n",
      "epoch 10\n",
      "Best validation accuracy! iteration:6500 accuracy: 70.48%\n",
      "Best validation accuracy! iteration:7000 accuracy: 70.58666666666667%\n",
      "epoch time 10.841922044754028\n",
      "epoch 11\n",
      "Best validation accuracy! iteration:7100 accuracy: 71.01333333333334%\n",
      "Best validation accuracy! iteration:7300 accuracy: 71.14666666666668%\n",
      "Best validation accuracy! iteration:7400 accuracy: 71.22666666666666%\n",
      "Best validation accuracy! iteration:7500 accuracy: 71.38666666666667%\n",
      "epoch time 11.611345529556274\n",
      "epoch 12\n",
      "Best validation accuracy! iteration:7800 accuracy: 71.44%\n",
      "Best validation accuracy! iteration:7900 accuracy: 72.02666666666667%\n",
      "epoch time 10.825202703475952\n",
      "epoch 13\n",
      "Best validation accuracy! iteration:8500 accuracy: 72.18666666666667%\n",
      "Best validation accuracy! iteration:8600 accuracy: 72.32%\n",
      "Best validation accuracy! iteration:8700 accuracy: 72.66666666666667%\n",
      "Best validation accuracy! iteration:8900 accuracy: 72.69333333333333%\n",
      "Best validation accuracy! iteration:9000 accuracy: 72.98666666666666%\n",
      "epoch time 11.577398538589478\n",
      "epoch 14\n",
      "Best validation accuracy! iteration:9300 accuracy: 73.22666666666666%\n",
      "Best validation accuracy! iteration:9400 accuracy: 73.28%\n",
      "Best validation accuracy! iteration:9600 accuracy: 73.62666666666667%\n",
      "epoch time 11.231267213821411\n",
      "epoch 15\n",
      "Best validation accuracy! iteration:9900 accuracy: 73.84%\n",
      "Best validation accuracy! iteration:10000 accuracy: 73.94666666666666%\n",
      "Best validation accuracy! iteration:10200 accuracy: 74.10666666666667%\n",
      "Best validation accuracy! iteration:10400 accuracy: 74.69333333333333%\n",
      "epoch time 11.502155542373657\n",
      "epoch 16\n",
      "Best validation accuracy! iteration:11000 accuracy: 74.88%\n",
      "epoch time 10.663375854492188\n",
      "epoch 17\n",
      "Best validation accuracy! iteration:11500 accuracy: 75.36%\n",
      "epoch time 10.654942512512207\n",
      "epoch 18\n",
      "Best validation accuracy! iteration:12000 accuracy: 75.62666666666667%\n",
      "Best validation accuracy! iteration:12100 accuracy: 75.97333333333333%\n",
      "Best validation accuracy! iteration:12400 accuracy: 76.18666666666667%\n",
      "Best validation accuracy! iteration:12600 accuracy: 76.32%\n",
      "epoch time 11.40597128868103\n",
      "epoch 19\n",
      "Best validation accuracy! iteration:12900 accuracy: 76.88%\n",
      "epoch time 10.658303022384644\n",
      "epoch 20\n",
      "Best validation accuracy! iteration:13900 accuracy: 77.25333333333333%\n",
      "epoch time 10.67508840560913\n",
      "epoch 21\n",
      "Best validation accuracy! iteration:14200 accuracy: 77.52%\n",
      "Best validation accuracy! iteration:14400 accuracy: 77.6%\n",
      "epoch time 10.99506688117981\n",
      "epoch 22\n",
      "Best validation accuracy! iteration:15300 accuracy: 77.70666666666666%\n",
      "Best validation accuracy! iteration:15400 accuracy: 77.86666666666667%\n",
      "epoch time 10.87771487236023\n",
      "epoch 23\n",
      "Best validation accuracy! iteration:15800 accuracy: 78.37333333333333%\n",
      "Best validation accuracy! iteration:16100 accuracy: 78.64%\n",
      "epoch time 10.833500862121582\n",
      "epoch 24\n",
      "epoch time 10.516992807388306\n",
      "epoch 25\n",
      "Best validation accuracy! iteration:17200 accuracy: 78.88%\n",
      "epoch time 10.651319026947021\n",
      "epoch 26\n",
      "Best validation accuracy! iteration:18200 accuracy: 79.22666666666666%\n",
      "epoch time 10.608006715774536\n",
      "epoch 27\n",
      "Best validation accuracy! iteration:18900 accuracy: 79.36%\n",
      "epoch time 10.65385365486145\n",
      "epoch 28\n",
      "Best validation accuracy! iteration:19200 accuracy: 79.57333333333334%\n",
      "Best validation accuracy! iteration:19600 accuracy: 79.6%\n",
      "epoch time 10.801466464996338\n",
      "epoch 29\n",
      "epoch time 10.385169744491577\n",
      "epoch 30\n",
      "Best validation accuracy! iteration:20400 accuracy: 79.73333333333333%\n",
      "Best validation accuracy! iteration:21000 accuracy: 79.86666666666667%\n",
      "epoch time 10.971332311630249\n",
      "epoch 31\n",
      "epoch time 10.37516713142395\n",
      "epoch 32\n",
      "Best validation accuracy! iteration:21900 accuracy: 80.16%\n",
      "Best validation accuracy! iteration:22000 accuracy: 80.48%\n",
      "epoch time 10.859502792358398\n",
      "epoch 33\n",
      "Best validation accuracy! iteration:22700 accuracy: 80.61333333333333%\n",
      "epoch time 10.683600187301636\n",
      "epoch 34\n",
      "Best validation accuracy! iteration:23400 accuracy: 80.64%\n",
      "Best validation accuracy! iteration:23600 accuracy: 80.66666666666667%\n",
      "epoch time 11.302732944488525\n",
      "epoch 35\n",
      "Best validation accuracy! iteration:24100 accuracy: 80.72%\n",
      "epoch time 10.703091621398926\n",
      "epoch 36\n",
      "Best validation accuracy! iteration:24700 accuracy: 81.01333333333334%\n",
      "epoch time 10.67256498336792\n",
      "epoch 37\n",
      "epoch time 10.45706844329834\n",
      "epoch 38\n",
      "Best validation accuracy! iteration:26100 accuracy: 81.06666666666666%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[20, 12, 12]]\n",
    "fc_units=[500, 250, 100, 100]\n",
    "conv_kernel_size = [[4, 4, 4]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=16,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[20, 12, 12]]\n",
    "fc_units=[1000, 500, 100, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=16,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[20, 16, 12]]\n",
    "fc_units=[500, 250, 100, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=16,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[20, 12, 12]]\n",
    "fc_units=[500, 250, 100, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=32,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[20, 16, 12]]\n",
    "fc_units=[500, 250, 100, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=32,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[20, 16, 12]]\n",
    "fc_units=[500, 250, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=16,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[32, 16, 12]]\n",
    "fc_units=[500, 250, 100, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=32,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[64, 32, 16]]\n",
    "fc_units=[500, 250, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=32,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# \n",
    "conv_featmap = [[64, 32, 16]]\n",
    "fc_units=[500, 250, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "\n",
    "\n",
    "my_training(X_train, y_train, X_val, y_val, outputSize = 5,\n",
    "         conv_featmap = conv_featmap,\n",
    "         fc_units = fc_units,\n",
    "         conv_kernel_size = conv_kernel_size,\n",
    "         pooling_size = pooling_size,\n",
    "         l2_norm=0.01,\n",
    "         seed=235,\n",
    "         learning_rate=1e-2,\n",
    "         epoch=50,\n",
    "         batch_size=16,\n",
    "         verbose=False,\n",
    "         pre_trained_model=None,\n",
    "         keepProbVal = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "#Already saved in my code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shapes after resizing: (11250, 32, 32, 3)\n",
      "pooling layer shape (?, 16, 16, 12) at pool 0\n",
      "Input shape for fully connected layer: (?, 3072)\n",
      "INFO:tensorflow:Restoring parameters from myCheckPoints/kaggleModel_1509632295.ckpt\n",
      "validation accuracy : 79.33333333333333%\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "from ecbm4040.neuralnets.kaggle import *\n",
    "# pre_trained_model = \"model/lenet_1509492956.data-00000-of-00001\"\n",
    "# pre_trained_model = \"model/lenet_1509492956.index\"\n",
    "# pre_trained_model = \"model/lenet_1509492956.meta\"\n",
    "# pre_trained_model = \"model/checkpoint\"\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val, \n",
    "outputSize = 5\n",
    "conv_featmap = [[20, 12, 12]]\n",
    "fc_units=[500, 250, 100]\n",
    "conv_kernel_size = [[5, 5, 5]]\n",
    "pooling_size=[2]\n",
    "l2_norm=0.01\n",
    "seed=235\n",
    "learning_rate=1e-2\n",
    "epoch=30\n",
    "batch_size=16\n",
    "verbose=False\n",
    "# pre_trained_model=None\n",
    "keepProbVal = 0.5\n",
    "\n",
    "modelCheckPoint = \"myCheckPoints/kaggleModel_1509632295.ckpt\"\n",
    "\n",
    "\n",
    "with tf.name_scope('inputs'):\n",
    "    xs = tf.placeholder(shape=[None,X_train.shape[1],X_train.shape[2], 3], dtype=tf.float32)\n",
    "    ys = tf.placeholder(shape=[None, ], dtype=tf.int64)\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "print(\"input shapes after resizing: {}\".format(X_train.shape))\n",
    "output, loss = my_LeNet(xs, ys,\n",
    "                     img_len=X_train.shape[1],\n",
    "                     channel_num=3,\n",
    "                     output_size=outputSize,\n",
    "                     conv_featmap=conv_featmap,\n",
    "                     fc_units=fc_units,\n",
    "                     conv_kernel_size=conv_kernel_size,\n",
    "                     pooling_size=pooling_size,\n",
    "                     l2_norm=l2_norm,\n",
    "                     seed=seed,\n",
    "                     keep_prob = keep_prob)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# # iters = int(X_train.shape[0] / batch_size)\n",
    "# # print('number of batches for training: {}'.format(iters))\n",
    "\n",
    "# step = train_step(loss)\n",
    "# eve = evaluate(output, ys)\n",
    "# pred = prediction(output)\n",
    "\n",
    "# iter_total = 0\n",
    "# best_acc = 0\n",
    "# cur_model_name = 'kaggleModel_{}'.format(int(time.time()))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    saver.restore(sess, modelCheckPoint)\n",
    "    \n",
    "#     saver = tf.train.import_meta_graph(pre_trained_model)\n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('model/'))\n",
    "#     graph = tf.get_default_graph()\n",
    "\n",
    "# #     idx = 0\n",
    "# #     tf_input = graph.get_operations()[idx].name+':0'\n",
    "    \n",
    "    \n",
    "    \n",
    "#     idx = 0\n",
    "#     tf_input = graph.get_operations()[idx].name+':0'\n",
    "#     output = graph.get_tensor_by_name('fc_layer_2/Add:0')\n",
    "#     pred = graph.get_tensor_by_name('evaluate/ArgMax:0')\n",
    "    eve = evaluate(output, ys)\n",
    "    pred = prediction(output)\n",
    "#     error_num = tf.count_nonzero(pred - ys, name='my_error_num')\n",
    "# #     cell_out = tf.add(tf.matmul(input_x, weight), bias)\n",
    "# #     print(graph.get_operations())\n",
    "# #     1/0\n",
    "# #     merge = tf.summary.merge_all()\n",
    "#     step = train_step(loss)\n",
    "# #     eve = evaluate(output, ys)\n",
    "#     pred = prediction(output)\n",
    "\n",
    "# #     writer = tf.summary.FileWriter(\"log/{}\".format(cur_model_name), sess.graph)\n",
    "# #     saver = tf.train.Saver()\n",
    "# #     sess.run(tf.global_variables_initializer())\n",
    "#     #X_val = tf.image.resize_images(X_val, [imageReshapeSize, imageReshapeSize])\n",
    "#     #X_val = X_val.eval(session=sess)\n",
    "#     # try to restore the pre_trained\n",
    "# #     try:\n",
    "# #         print(\"Load the model from: {}\".format(pre_trained_model))\n",
    "# #         saver.restore(sess, pre_trained_model)\n",
    "# #     except Exception:\n",
    "# #         print(\"Load model Failed!\")\n",
    "# #         pass\n",
    "    \n",
    "    valid_eve = sess.run(eve, feed_dict={xs: X_val, ys: y_val,\n",
    "                                                               keep_prob: 1.0})\n",
    "    valid_acc = 100 - valid_eve * 100 / y_val.shape[0]\n",
    "    print('validation accuracy : {}%'.format(valid_acc))\n",
    "    myPreds = sess.run(pred, feed_dict={xs: X_test,\n",
    "                                                               keep_prob: 1.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 2, 0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3071"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_eve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate .csv file for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code snippet can be used to generate your prediction .csv file.\n",
    "\n",
    "import csv\n",
    "with open('predicted.csv','w') as csvfile:\n",
    "    fieldnames = ['Id','label']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()    \n",
    "    for index,l in enumerate(myPreds):\n",
    "        filename = str(index)+'.png'\n",
    "        label = str(l)\n",
    "        writer.writerow({'Id': filename, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
